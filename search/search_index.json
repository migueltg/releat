{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ReLeAT","text":"<p>REinforcement LEarning for Algorithmic Trading is a python framework for learning medium frequency trading algorithms for MetaTrader 5 (other trading platforms are planned for the future).</p> <p>DISCALIMER: The information provided herein is for educational and informational purposes only and should not be construed as financial advice. It is not a recommendation to trade or invest real money. Always exercise your own judgment and use common sense when making financial decisions.</p>"},{"location":"#vision","title":"Vision","text":"<p>To build a collaborative community where software engineers, data scientists, RL researchers, quants and finance and economic experts can share knowledge. This framework covers the end-to-end process including:</p> <ul> <li>extracting data from a MetaTrader5</li> <li>building custom features from tick data</li> <li>gym environment factory to simulate the trading environment</li> <li>training a reinforcement learning and/or machine learning algorithms (Tensorflow)</li> <li>deploying trained models</li> <li>executing trades</li> </ul> <p>In progress:</p> <ul> <li>additional platforms including Interactive Brokers and Binance</li> <li>custom features for candle data and macroeconomic events</li> <li>incoporate other deep learning frameworks such as PyTorch</li> <li>better sofware development practices, CI/CD, MLOps, tests</li> <li>examples for deployment to cloud to AWS and GCP</li> <li>monitoring and observability</li> </ul>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>A single container for developing, training, deploying and trading for MetaTrader5 for Linux and Windows (via WSL)</li> <li>A simple command line interface to orchestrate the end-to-end process.</li> <li>Configuration files that define each step for a specific agent. These are structured to facilitate rapid experimentation and easy integration with Ray's Tune module. types, etc.</li> <li>Focuses on Medium Frequency Trading strategies (&gt;1 second and &lt;1 day) using tick data as the input for each step. General latency of the system is ~0.1-3s depending on the complexity of feature engineering and model and resources available.</li> <li>In contrast to most other python packages that focus on a deep coverage on one part of the algorithmic trading process, this framework focuses on rapid experimentation lifecycles from idea to deploying and tracking paper trades.</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>Note this is still a work in progress.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - instructions on how to build or download the docker container</li> <li>Basic Usage - an example on how to build a feature set, train, deploy and trade a RL strategy for EURUSD on a metaquotes demo account</li> <li>Architecture - a high level overview of the conceptual architecture and repository structure</li> </ul>"},{"location":"#examples","title":"Examples","text":"<p>A collection of jupyter notebooks to show how different components work. Note that these notebooks are stored in <code>docs/examples</code></p>"},{"location":"#development-notes","title":"Development Notes","text":"<ul> <li>Containerisation - detailed explanation on the design choices for the DockerFile</li> </ul>"},{"location":"#troubleshooting","title":"Troubleshooting","text":"<p>Troubleshooting provides guidance on some the common issues that might arise</p>"},{"location":"#contributing","title":"Contributing","text":"<p>ReLeAT is an open-source project and we're always looking for contributors and collaborators to make this project even better! Contribution Guidelines are in progress.</p>"},{"location":"#license","title":"License","text":"<p>ForexRL is distributed under the MIT License. Feel free to use, modify, and share the library according to the terms outlined in the license.</p>"},{"location":"development_notes/mt5_api/","title":"Mt5 api","text":"<p>winedbg --command \"info proc\"</p> <p>from setproctitle import setproctitle, setthreadtitle setproctitle(f\"mt5_api_{symbol}\") setthreadtitle(f\"mt5_api_{symbol}\")</p> <p>https://sentry.io/answers/flask-getting-post-data/</p> <p>https://www.mql5.com/en/articles/5691</p>"},{"location":"development_notes/releat_dockerfile/","title":"Building the ReLeAT DockerFile","text":"<p>Summary of design considerations for the Dockerfile that has the following features:</p> <ul> <li>Install and run MetaTrader5 and python in wine</li> <li>Rllib in linux container with Nividia GPU enabled</li> <li>Poetry to manage package dependency</li> <li>Aerospike as databse tool</li> </ul>"},{"location":"development_notes/releat_dockerfile/#key-points","title":"Key Points","text":"<ul> <li> <p>Check aerospike versions because they tend to be updated quite frequently</p> </li> <li> <p>User the repo folder as the context when building and running</p> </li> <li> <p>On first start up of MetaTrader5, you need to manually click on the 'Accounts' button to allow it to connect to broker servers. After logging in you need to manually click the 'Allow Autotrading' button - slightly weird behaviour from running MetaTrader5 on wine.</p> </li> <li> <p>Future work - updating packages to latest version</p> </li> </ul>"},{"location":"development_notes/releat_dockerfile/#design-considerations","title":"Design Considerations","text":""},{"location":"development_notes/releat_dockerfile/#base-layer","title":"Base layer","text":"<ul> <li> <p>Ubuntu 20.04 was used as the base image - could upgrade in the future.</p> </li> <li> <p><code>DEBIAN_FRONTEND=noninteractive</code> to accept all default options when installing and removing software (i.e. apt-get install). This prevents questions from blocking the installation process</p> </li> <li> <p>Set environment paths so that Miniconda, MetaTrader5 and python environments are saved in the correct place and/or easily searchable.</p> </li> <li> <p><code>MT5_PATH</code> is saved as an environment variable because the entrypoint script check whether this file exists. If not, then it will install MT5</p> </li> <li> <p>Set environment display to 0 to pass through screen. Not sure if this is necessary. But some wine windows apps (i.e. Python and MetaTrader5) cannot be installed headlessly</p> </li> <li> <p>Tensorflow version is set to 2.11.0 - could be upgraded in the future</p> </li> <li> <p>Python version is set to 2.10.10 - couldn't upgrade this to 2.11 because at the time, the aerospike python package and rllib was incompatible / not stable</p> </li> </ul> <pre><code>FROM ubuntu:focal\n\nENV DEBIAN_FRONTEND=noninteractive\nENV TZ=Etc/UTC\n\nENV PATH=\"${PATH}:/root/miniconda3/bin:/root/.local/bin\"\n# Location of linux python env\nENV PY_PATH=\"./.venv\"\n# Metatrader path\nENV MT5_PATH=\"/root/.wine/drive_c/Program Files/MetaTrader 5/terminal64.exe\"\n\nENV DISPLAY :0\n\n# Tensorflow version\nENV TF_V=\"2.11.0\"\n# Python version\nENV PY_V=\"3.10.10\"\n</code></pre>"},{"location":"development_notes/releat_dockerfile/#wine","title":"Wine","text":"<p>Wine enables the running of Windows software on Linux systems. For other options, see this guideline for installing and running wine.</p> <ul> <li> <p><code>WINEDLLOVERRIDES=\"mscoree,mshtml=\"</code> disables the Mono installer dialog allowing it to be installed in the Dockerfile headlessly instead of in the entrypoint script.</p> </li> <li> <p><code>WINEDEBUG=\"fixme-all,err-all\"</code> suppresses most of the warnings, reducing log volume</p> </li> <li> <p><code>WINEPREFIX=\"/root/.wine\"</code> helps processes find the installation location of wine</p> </li> <li> <p><code>dpkg --add-architecture i386</code> enables the installation of multiarch binaries</p> </li> <li> <p>install development version of wine - could also use stable</p> </li> <li> <p><code>winecfg -v win10</code> sets the windows 10, necessary for python and MT5</p> </li> <li> <p><code>wineserver -w</code> waits until currently running wineserver terminates. Not sure if this is necessary</p> </li> </ul> <pre><code># Wine configs\nENV WINEDLLOVERRIDES=\"mscoree,mshtml=\"\nENV WINEDEBUG=\"fixme-all,err-all\"\nENV WINEPREFIX=\"/root/.wine\"\n\n\n# Install wine\nRUN apt-get update \\\n&amp;&amp; apt-get install -y wget gnupg2 dialog apt-utils software-properties-common curl make git tzdata \\\n&amp;&amp; dpkg --add-architecture i386 \\\n&amp;&amp; mkdir -pm755 /etc/apt/keyrings \\\n&amp;&amp; wget -O /etc/apt/keyrings/winehq-archive.key https://dl.winehq.org/wine-builds/winehq.key \\\n&amp;&amp; wget -nc https://dl.winehq.org/wine-builds/ubuntu/dists/focal/winehq-focal.sources \\\n&amp;&amp; mv winehq-focal.sources /etc/apt/sources.list.d/ \\\n&amp;&amp; apt-get update \\\n&amp;&amp; apt-get install -y --install-recommends winehq-devel \\\n&amp;&amp; apt-get remove -y winbind \\\n&amp;&amp; apt-get install -y winbind \\\n&amp;&amp; rm -rf /var/lib/apt/lists/* /winehq.key \\\n&amp;&amp; winecfg -v win10 \\\n&amp;&amp; wineserver -w\n</code></pre> <p>Install mono and gecko, wine packages that a necessary to make windows applications work. Depending on what version of wine is installed, install the appropriate mono version and gecko version. Reboot and remove installation files once installed</p> <pre><code># Install wine mono and gecko\nRUN wget https://dl.winehq.org/wine/wine-mono/8.0.0/wine-mono-8.0.0-x86.msi \\\n&amp;&amp; wget https://dl.winehq.org/wine/wine-gecko/2.47.4/wine-gecko-2.47.4-x86.msi \\\n&amp;&amp; wget https://dl.winehq.org/wine/wine-gecko/2.47.4/wine-gecko-2.47.4-x86_64.msi \\\n&amp;&amp; wineboot \\\n&amp;&amp; wine wine-mono-8.0.0-x86.msi /quiet \\\n&amp;&amp; wine wine-gecko-2.47.4-x86.msi /quiet \\\n&amp;&amp; wine wine-gecko-2.47.4-x86_64.msi /quiet \\\n&amp;&amp; rm wine-mono-8.0.0-x86.msi wine-gecko-2.47.4-x86.msi wine-gecko-2.47.4-x86_64.msi\n</code></pre>"},{"location":"development_notes/releat_dockerfile/#conda-and-poetry","title":"Conda and Poetry","text":"<p>Miniconda and poetry for manage python packages. Both are required because of GPU. Miniconda is used to install nvidia packages whilst poetry is for pure python. <code>eval \"$(conda shell.bash hook)\"</code> sets up shell functions for Conda.</p> <pre><code># Install Miniconda and Poetry\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n&amp;&amp; mkdir /root/.conda \\\n&amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b \\\n&amp;&amp; rm -f Miniconda3-latest-Linux-x86_64.sh \\\n&amp;&amp; eval \"$(conda shell.bash hook)\" \\\n&amp;&amp; curl -sSL https://install.python-poetry.org | python3 -\n</code></pre>"},{"location":"development_notes/releat_dockerfile/#aerospike","title":"Aerospike","text":"<p>Aerospike is a real-time in memory database, which will be used to for storing observations for the reinforment learning training process. This is installed last because version are updated frequently, so we only need to rebuild the last layer.</p> <p>Note: check aerospike versions regularly</p> <pre><code># aerospike server version\nARG AS_V=\"6.4.0.1\"\n# aerospike tool version\nARG AT_V=\"9.0.0\"\n# Installing aerospike\nRUN wget -O aerospike.tgz https://download.aerospike.com/artifacts/aerospike-server-community/${AS_V}/aerospike-server-community_${AS_V}_tools-${AT_V}_ubuntu20.04_x86_64.tgz \\\n&amp;&amp; tar -xvf aerospike.tgz \\\n&amp;&amp; cd aerospike-server-community_${AS_V}_tools-${AT_V}_ubuntu20.04_x86_64 \\\n&amp;&amp; ./asinstall \\\n&amp;&amp; cd .. \\\n&amp;&amp; rm aerospike.tgz\n\nENTRYPOINT [\"/bin/bash\"]\n</code></pre>"},{"location":"development_notes/releat_dockerfile/#entrypoint","title":"Entrypoint","text":"<p>The default entrypoint is the bash terminal. An entrypoint script is included in that folder and run depending on how you plan on accessing the container. The next section explains each component of the entrypoint script, followed by the different ways to build and run the container</p>"},{"location":"development_notes/releat_dockerfile/#entrypoint-logic","title":"Entrypoint logic","text":"<ul> <li><code>#!/bin/bash</code> - when using VSCode's devcontainer functionality must use bash because sh doesn't work, not sure why</li> </ul>"},{"location":"development_notes/releat_dockerfile/#linux-python-environment","title":"Linux python environment","text":"<ul> <li> <p>check if <code>./.venv</code> folder already exists, if so we can skip the installation of python libraries</p> </li> <li> <p>Create python environment. Note this is done in entrypoint rather than Dockerfile so that the installed files are in the same location as the repo within the binded mount. Works well for developing in VSCode's devcontainer, need to test more when using other IDEs.</p> </li> <li> <p>Currently we're still using an old version of tensorflow - see notes below for future version for python</p> </li> <li> <p>For GPU usage, we need conda to install cuda packages. Then everything else can be installed by pip</p> </li> <li> <p>Make sure to lock tensorflow version in poetry lock file, then install all other packages using poetry</p> </li> </ul> <pre><code>if test -d \"./.venv\"; then\necho \"Conda environments installed\"\nelse\necho \"install linux packages\"\nconda create --prefix ${PY_PATH} python=${PY_V} -y\n    eval \"$(conda shell.bash hook)\"\nconda activate ${PY_PATH}\n\n# Install linux tensorflow=2.11.0\nconda install -c conda-forge cudatoolkit=11.2.2 cudnn=8.1.0 -y\n    export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/\n    mkdir -p $CONDA_PREFIX/etc/conda/activate.d\n    echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' &gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh \\\nLD_LIBRARY_PATH=/content/conda-env/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n    pip install --upgrade pip\n    pip install tensorflow==${TF_V}\n\n# Install linux python packages\nconda install -c anaconda -y\n    poetry init --python=~${PY_V}\npoetry add --lock tensorflow=${TF_V}\npoetry install --with rl\n    pre-commit install\n\nfi\n</code></pre>"},{"location":"development_notes/releat_dockerfile/#wine-packages","title":"Wine packages","text":"<ul> <li> <p>Install same version of python in wine. We need this to programmatically extract data and trade in MT5</p> </li> <li> <p>Install python packages via poetry, note this means that when we deploy tensorflow models, we will be using cpu version only. Currently designed to be a monolith to simplify networking between applications / container. Might look at a microservice design in the future</p> </li> <li> <p>Needs to be a fresh install each time for the dev container because the gui works right after install, however doesn't work when you close and re-open vscode. possibly a wine / linux / wsl issue</p> </li> <li> <p>Keep install files for now because deleting them seems to cause an error - I suspect some kind of race condition where the file is deleted before install is complete.</p> </li> <li> <p>Other references: https://github.com/python-poetry/poetry/issues/5037</p> </li> <li> <p>Install MT5</p> </li> </ul> <pre><code># Installing wine python\necho \"installing wine python\" \\\n&amp;&amp; wget https://www.python.org/ftp/python/${PY_V}/python-${PY_V}-amd64.exe \\\n&amp;&amp; wine python-${PY_V}-amd64.exe /quiet InstallAllUsers=1 PrependPath=1 Include_test=0 \\\n# &amp;&amp; rm python-${PY_V}-amd64.exe \\\n&amp;&amp; echo \"Python installed successfully\"\n\n# Installing wine python packages\necho \"installing python packages\" \\\n&amp;&amp; wine pip install poetry \\\n&amp;&amp; wine poetry config virtualenvs.create false \\\n&amp;&amp; wine poetry config virtualenvs.in-project false \\\n&amp;&amp; wine poetry install --only mt5\n\n# Installing MT5\necho \"Installing MT5\" \\\n&amp;&amp; wget https://download.mql5.com/cdn/web/metaquotes.software.corp/mt5/mt5setup.exe \\\n&amp;&amp; wine mt5setup.exe /auto \\\n# &amp;&amp; rm mt5setup.exe \\\n&amp;&amp; echo \"MT5 installed successfully\"\n\nfi\n</code></pre>"},{"location":"development_notes/releat_dockerfile/#side-notes","title":"Side notes","text":"<ul> <li>Future versions of tensorflow should be installed like this:</li> </ul> <pre><code>    conda install -c conda-forge cudatoolkit=11.8.0\n    python3 -m pip install nvidia-cudnn-cu11==8.6.0.163 tensorflow==2.13.0\n    mkdir -p $CONDA_PREFIX/etc/conda/activate.d\n    echo 'CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n    echo 'export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$CONDA_PREFIX/lib/:$LD_LIBRARY_PATH' &gt;&gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n    source $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n</code></pre> <ul> <li>start aerospike - may leave this for later because we may want to put aerospike configs into the folder of each agent / experiment</li> </ul> <pre><code>start aerospike\nasd --config-file ./infrastructure/aerospike.conf\n</code></pre>"},{"location":"development_notes/releat_dockerfile/#building-the-container","title":"Building the Container","text":"<p>Using the repo folder as your context / current working directory, the DockerFile can be built by the following command.</p> <pre><code>docker build -t releat -f ./infrastructure/releat/Dockerfile .\n</code></pre>"},{"location":"development_notes/releat_dockerfile/#running-the-container","title":"Running the Container","text":"<p>This container can be used for development in 2 main ways:</p> <ul> <li> <p>VSCode's devcontainer functionality</p> </li> <li> <p>docker container with a mount</p> </li> </ul>"},{"location":"development_notes/releat_dockerfile/#vscode-devcontainer","title":"VSCode devcontainer","text":"<p>If you use VSCode as your IDE, you can use the Dev Container extension, using the provided specification in the <code>.devcontainer</code> folder. See the official guide on how to start it up.</p> <ul> <li> <p>Note that in the <code>.devcontainer/devcontainer.json</code>, we invoke the entrypoint script after the docker image has been started up to install the project and MetaTrader5</p> </li> <li> <p>Upon first open, you need to click autotrading + click add account in order for MT5 to connect to servers, otherwise it just hangs</p> </li> <li> <p>If the wine gui is frozen or you can't click on buttons or resizing windows causes distortion, restart your linux or wsl machine or container</p> </li> </ul> <p>Side Notes:</p> <ul> <li>Future low priority work to make it more stable: If Developing on Windows system via WSL, to pass through Metatrader GUI, follow this guide and this discussion</li> </ul>"},{"location":"development_notes/releat_dockerfile/#docker-run","title":"docker run","text":"<p>Assumes your context / current working directory is the repo folder. We can build the docker image by:</p> <pre><code>docker run \\\n--net host \\\n-v /tmp/.X11-unix:/tmp/.X11-unix \\\n-v $(pwd):/releat \\\n-e DISPLAY \\\n-it \\\n--name releat \\\n--gpus=all \\\nreleat215/releat:1.0 \\\n-c './releat/infrastructure/releat/entrypoint.sh &amp;&amp; cd releat &amp;&amp; /bin/bash'\n</code></pre> <ul> <li> <p><code>--net host</code> container shares network with the host, meaning we don't need to manually listen to ports - i think this also allows the container to access the internet</p> </li> <li> <p><code>-v /tmp/.X11-unix:/tmp/.X11-unix</code> passes through the display. Necessary because MT5 can't be run headlessly (without lots of tinkering)</p> </li> <li> <p><code>-v $(pwd):/releat</code> mounts</p> </li> <li> <p><code>-e DISPLAY</code> Maybe its necessary to pass through the display</p> </li> <li> <p><code>-v $(pwd):/releat</code> mounts the repo to the corresponding location in the container</p> </li> <li> <p><code>--gpus=all</code> passes through GPUs from your machine to the container</p> </li> <li> <p>Still need add in code that binds the location of the repo / fix this docker run code</p> </li> </ul>"},{"location":"development_notes/resources/","title":"Resources","text":""},{"location":"development_notes/resources/#data","title":"Data","text":"<p>10 years of dukascopy tick data 2008-2019</p>"},{"location":"development_notes/architecture/overview/","title":"Architecture","text":""},{"location":"development_notes/architecture/overview/#conceptual-architecture","title":"Conceptual Architecture","text":"<p>The following diagram is a high level outline of the key components and their interactions.</p> <p></p>"},{"location":"development_notes/architecture/overview/#connectors","title":"Connectors","text":"<p>Processes that extract and load data from internal and external applications. For example:</p> <ul> <li>AWS services, i.e. s3</li> <li>extract data from trading platforms, i.e. MT5, IBKR etc.</li> <li>load / save data into s3 etc.</li> <li>apis to transmit data between processes / instances</li> </ul>"},{"location":"development_notes/architecture/overview/#feature-engineering","title":"Feature engineering","text":"<p>Transforms raw data to model inputs.</p> <p>Raw data includes:</p> <ul> <li>tick data</li> <li>candles</li> <li>announcement data (TBD)</li> </ul> <p>Outputs:</p> <ul> <li>feature that are ingested into the ml and rl models</li> </ul> <p>Processes:</p> <ul> <li>batch for fast prototype</li> <li>real-time for inference</li> </ul>"},{"location":"development_notes/architecture/overview/#reinforcement-agent","title":"Reinforcement Agent","text":"<p>Reinforcement agent consists of 4 major components:</p> <ul> <li>gym environment:  Simulated trading environnment for training the agent. Further details can be found in the trading environment section</li> <li>model: Neural network used to generate predictions. Further imformation can be found in models section</li> <li>reinforcement learning algorithm: Policy and training process that defines how agents learn</li> <li>hyperparameter tuning: Application of ray.tune to experiment and optimise hyperparameters</li> </ul>"},{"location":"development_notes/architecture/overview/#trading-execution","title":"Trading Execution","text":"<p>Logic that ingests the signals generated by a deployed rl algorithm to make trades to one or more brokers. This includes:</p> <ul> <li>ingesting multiple trading signals</li> <li>ensembling logic to combine signals</li> <li>interacts with brokers</li> </ul>"},{"location":"development_notes/architecture/overview/#utility-functions","title":"Utility Functions","text":"<p>Various utility functions to enable trading including:</p> <ul> <li>infrastructure - cloud formation templates and configuration files for other applications</li> <li>deployment - functions to facilitate deploying of models and data collection processes</li> <li>apis - apis to send data between instances / processes</li> <li>configs - functions to store and interpret the configuration files and parameters</li> </ul>"},{"location":"development_notes/architecture/overview/#repository-structure","title":"Repository structure","text":"<p>The repository structureis broadly aligned with the conceptual architecture:</p> <pre><code>agents/             configuration files for an agent, from tick data to\n\u2502                   recommendation of trade / hold\n\u2502\napis/               api wrappers to expose functions internally and externally\n\u2502\ndata/               data and models generated by these functions are stored here\n\u2502\ndocs/\n\u2502\n\u251c\u2500\u2500 examples/       Jupyter notebooks to explain how individual components works\n\u2502\ninfrastructure/     templates, configurations, dockerfile, cloudformation\n\u2502                   templates, etc.\n\u2502\nreleat/\n\u2502\n\u251c\u2500\u2500 connectors/     Functions for communicating with external apps\n\u2502\n\u251c\u2500\u2500 data/           Functions extracting data and transforming tick to model\n\u2502   \u2502               features\n\u2502   \u2502\n|   \u251c\u2500\u2500 simple/     Functions that apply one set of transforms to the tick\n|   |               data of one trading instrument\n|   |\n\u2502   \u2514\u2500\u2500 complex/    Functions that combine multiple simple features\n\u2502\n\u251c\u2500\u2500 gym_env/        Defines and specifies the simulated trading environment\n|                   that the RL agent will be trained on\n\u2502\n\u251c\u2500\u2500 models/         Building blocks for the neural network for the RL algorithm\n\u2502   \u2502\n|   \u251c\u2500\u2500 pytorch/\n\u2502   \u2514\u2500\u2500 tf/\n\u2502\n\u251c\u2500\u2500 signals/        Deploys one agent as defined in the agents directory\n\u2502\n\u251c\u2500\u2500 trader/         Logic to combine multiple signals and execute trades\n\u2502\n\u251c\u2500\u2500 utils/          General utility functions\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 configs/    Key directory for processing configuration files in the\n|                   agents folder + stores constants used throughout the repo\n\u2502\n\u251c\u2500\u2500 vizualization/  Visualisation tools\n\u2502\n\u2514\u2500\u2500 workflows/      Scripts for running tasks, i.e. building features, etc.\n</code></pre>"},{"location":"examples/eval_rl_agent_simple/","title":"Evaluate simple RL agent (t00001)","text":"In\u00a0[1]: Copied! <pre>import ray\nfrom ray.rllib.connectors.agent.obs_preproc import ObsPreprocessorConnector\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.policy.sample_batch import SampleBatch\nfrom ray.rllib.utils.typing import AgentConnectorDataType\nfrom releat.utils.configs.config_builder import load_config\nfrom releat.gym_env.gym_env import FxEnv\nfrom ray.rllib.algorithms.impala import ImpalaConfig as RLAlgorithmConfig\nfrom glob import glob\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport numpy as np\nfrom releat.gym_env.action_processor import make_action_labels, build_action_map, build_pos_arrs\nimport pandas as pd\nimport aerospike\nfrom releat.data.pipeline import update_gym_env_hparam\nimport os\n</pre> import ray from ray.rllib.connectors.agent.obs_preproc import ObsPreprocessorConnector from ray.rllib.models import ModelCatalog from ray.rllib.policy.sample_batch import SampleBatch from ray.rllib.utils.typing import AgentConnectorDataType from releat.utils.configs.config_builder import load_config from releat.gym_env.gym_env import FxEnv from ray.rllib.algorithms.impala import ImpalaConfig as RLAlgorithmConfig from glob import glob from tqdm import tqdm from copy import deepcopy import numpy as np from releat.gym_env.action_processor import make_action_labels, build_action_map, build_pos_arrs import pandas as pd import aerospike from releat.data.pipeline import update_gym_env_hparam import os In\u00a0[9]: Copied! <pre># load configuration and neural network\nconfig, AgentModel = load_config('t00001',enrich_feat_spec=True,is_training=False,load_model=True)\n\n# register model in ray\nModelCatalog.register_custom_model(\"AgentModel\", AgentModel)\n\n# initialise ray in  local mode, i.e. not connect to the running ray instance\nray.init(local_mode=True)\n\n# log directory\nlogdir = config.paths.algo_dir\n\n# gym environment config in a format required by rllib's trainer\nenv_config = {\n    **config.rl_env,\n    \"env_config\": dict(config),\n}\n\n# initialise trainer\ntrainer = (\n    RLAlgorithmConfig()\n    .training(**config.rl_train)\n    .environment(env=FxEnv, **env_config)\n    .framework(**config.rl_framework)\n    .rollouts(**config.rl_rollouts)\n    .exploration(**config.rl_explore)\n    .reporting(**config.rl_reporting)\n    .debugging(\n        logger_config={\"type\": \"ray.tune.logger.TBXLogger\", \"logdir\": logdir},\n        **config.rl_debug,\n    )\n    .resources(**config.rl_resources)\n    .build()\n)\n\n# print the neural network model used in this example\ntrainer.get_policy().model.base_model.summary()\n</pre> # load configuration and neural network config, AgentModel = load_config('t00001',enrich_feat_spec=True,is_training=False,load_model=True)  # register model in ray ModelCatalog.register_custom_model(\"AgentModel\", AgentModel)  # initialise ray in  local mode, i.e. not connect to the running ray instance ray.init(local_mode=True)  # log directory logdir = config.paths.algo_dir  # gym environment config in a format required by rllib's trainer env_config = {     **config.rl_env,     \"env_config\": dict(config), }  # initialise trainer trainer = (     RLAlgorithmConfig()     .training(**config.rl_train)     .environment(env=FxEnv, **env_config)     .framework(**config.rl_framework)     .rollouts(**config.rl_rollouts)     .exploration(**config.rl_explore)     .reporting(**config.rl_reporting)     .debugging(         logger_config={\"type\": \"ray.tune.logger.TBXLogger\", \"logdir\": logdir},         **config.rl_debug,     )     .resources(**config.rl_resources)     .build() )  # print the neural network model used in this example trainer.get_policy().model.base_model.summary()    In\u00a0[3]: Copied! <pre># restore latest checkpoint\nfiles = sorted(glob(f\"{logdir}/checkpoint*\"))\ncheckpoint_f = sorted(glob(f\"{files[-1]}/*\"))\ntrainer.restore(checkpoint_f[0])\n</pre> # restore latest checkpoint files = sorted(glob(f\"{logdir}/checkpoint*\")) checkpoint_f = sorted(glob(f\"{files[-1]}/*\")) trainer.restore(checkpoint_f[0]) <pre>2023-11-09 11:09:50,335\tINFO trainable.py:918 -- Restored on 192.168.65.3 from checkpoint: /workspaces/releat/data/agent/t00001/algo/checkpoint_000924\n2023-11-09 11:09:50,336\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 924, '_timesteps_total': None, '_time_total': 30689.187471151352, '_episodes_total': 30298}\n</pre> In\u00a0[22]: Copied! <pre># update hyperparameters - this is necessary if you've made any changes to \n# the agent_config.py since gym reads off aerospike rather than the python file\nclient = aerospike.client(config.aerospike.connection).connect()\n_ = update_gym_env_hparam(config, client)\n\n# print out hyperparameters stored in aerospike\n# meta_data_key = (\n#             config.aerospike.namespace,\n#             f\"{config.aerospike.set_name}_hparams\",\n#             \"gym_env_configs\",\n#         )\n# _, _, bins = client.get(meta_data_key)\n# bins['max_samples']\n# bins['rec_ep_t']\n</pre> # update hyperparameters - this is necessary if you've made any changes to  # the agent_config.py since gym reads off aerospike rather than the python file client = aerospike.client(config.aerospike.connection).connect() _ = update_gym_env_hparam(config, client)  # print out hyperparameters stored in aerospike # meta_data_key = ( #             config.aerospike.namespace, #             f\"{config.aerospike.set_name}_hparams\", #             \"gym_env_configs\", #         ) # _, _, bins = client.get(meta_data_key) # bins['max_samples'] # bins['rec_ep_t'] <pre>2023-11-09 15:07:45,552  INFO   releat.data.pipeline  |  gym config and hparams updated - total recs: 198659\n</pre> In\u00a0[5]: Copied! <pre># initialise the gym environment\nenv = FxEnv(dict(config))\n\n# action map is a 9 x 4 array to help map the gyms predicted action (an integer \n# from 0 - 8) to an action, i.e. open or close a long or short position\naction_map = build_action_map(env.trader)\n\n# the gym representation of an opened trade\ngym_portfolio = build_pos_arrs(env.trader)\n\n# action labels, the string representation of what each action means\naction_labels = make_action_labels(config, action_map, gym_portfolio)\ndisplay(action_labels)\n</pre> # initialise the gym environment env = FxEnv(dict(config))  # action map is a 9 x 4 array to help map the gyms predicted action (an integer  # from 0 - 8) to an action, i.e. open or close a long or short position action_map = build_action_map(env.trader)  # the gym representation of an opened trade gym_portfolio = build_pos_arrs(env.trader)  # action labels, the string representation of what each action means action_labels = make_action_labels(config, action_map, gym_portfolio) display(action_labels) <pre>['Hold',\n 'EURUSD_Short_Open_1_0',\n 'EURUSD_Short_Close_1_0',\n 'EURUSD_Short_Open_2_0',\n 'EURUSD_Short_Close_2_0',\n 'EURUSD_Long_Open_1_0',\n 'EURUSD_Long_Close_1_0',\n 'EURUSD_Long_Open_2_0',\n 'EURUSD_Long_Close_2_0']</pre> <p>Each component of the action label:</p> <ul> <li><code>EURUSD</code> is the trading instrument</li> <li><code>Short</code> or <code>Long</code> indicates the direction of the trade</li> <li><code>1</code> or <code>2</code> indicates the number of lots to trade</li> <li><code>0</code> indicates the portfolio index (always 0 in this case, because the dimension of axis 1 only has length 1)</li> </ul> In\u00a0[6]: Copied! <pre>actions_pos = []\nactions_no_pos = []    \nval_pos = []\nval_no_pos = []\n\n# reset environment\nobs, _ = env.reset()\n\nwith trainer.get_policy().get_session().as_default():\n    \n    # for calculating the value function\n    pp = trainer.get_policy().agent_connectors[ObsPreprocessorConnector][0]\n    value_func = trainer.get_policy().model.value_function()\n    \n    for i in tqdm(range(config.gym_env.eval_len)):\n        \n        # get action when the portfolio is taken into account\n        act_pos, _, act_prob_pos = trainer.compute_single_action(\n            obs,\n            explore=False,\n            full_fetch=True,\n        )\n\n        # calculate value function when portfolio is taken into account\n        input_dict = {SampleBatch.OBS: obs}\n        acd = AgentConnectorDataType(\"0\", \"0\", input_dict)\n        pp.reset(env_id=\"0\")\n        ac_o = pp([acd])[0]\n        observation = ac_o.data[SampleBatch.OBS]\n        input_dict = {\n            \"default_policy/obs:0\": observation.reshape((1, -1)),\n        }\n        v = value_func.eval(input_dict)[0]\n        val_pos.append(v)\n\n\n        # calculate action as if the agent had not opened a trade\n        # note that for this example, this is used for analysis rather than for\n        # deciding whether to open or close a position\n        obs_no_pos = deepcopy(obs)\n        obs_no_pos[\"pos_val\"] = np.array([[0.0,0.0]]*20, dtype=np.float32)\n        m = [1 if \"Open\" in x else 0 for x in action_labels]\n        m[0] = 1\n        obs_no_pos[\"mask\"] = np.array(m, dtype=np.float32)\n        act_no_pos, _, act_prob_no_pos = trainer.compute_single_action(\n            obs_no_pos,\n            explore=False,\n            full_fetch=True,\n        )\n\n        # calculate value function as if agent had not opened a trade\n        # note that for this example, this is used for analysis rather than for\n        # deciding whether to open or close a position\n        input_dict = {SampleBatch.OBS: obs_no_pos}\n        acd = AgentConnectorDataType(\"0\", \"0\", input_dict)\n        pp.reset(env_id=\"0\")\n        ac_o = pp([acd])[0]\n        observation = ac_o.data[SampleBatch.OBS]\n        input_dict = {\n            \"default_policy/obs:0\": observation.reshape((1, -1)),\n        }\n        v = value_func.eval(input_dict)[0]\n        val_no_pos.append(v)\n\n        # take step based on action that takes positions into account\n        obs, reward, done, _, _ = env.step(act_pos)\n        \n        # store log-likelihoods\n        actions_pos.append(act_prob_pos[\"action_dist_inputs\"])\n        actions_no_pos.append(act_prob_no_pos[\"action_dist_inputs\"])\n\n        if done:\n            break\n\n\n    res, cp = env.get_results()\n</pre> actions_pos = [] actions_no_pos = []     val_pos = [] val_no_pos = []  # reset environment obs, _ = env.reset()  with trainer.get_policy().get_session().as_default():          # for calculating the value function     pp = trainer.get_policy().agent_connectors[ObsPreprocessorConnector][0]     value_func = trainer.get_policy().model.value_function()          for i in tqdm(range(config.gym_env.eval_len)):                  # get action when the portfolio is taken into account         act_pos, _, act_prob_pos = trainer.compute_single_action(             obs,             explore=False,             full_fetch=True,         )          # calculate value function when portfolio is taken into account         input_dict = {SampleBatch.OBS: obs}         acd = AgentConnectorDataType(\"0\", \"0\", input_dict)         pp.reset(env_id=\"0\")         ac_o = pp([acd])[0]         observation = ac_o.data[SampleBatch.OBS]         input_dict = {             \"default_policy/obs:0\": observation.reshape((1, -1)),         }         v = value_func.eval(input_dict)[0]         val_pos.append(v)           # calculate action as if the agent had not opened a trade         # note that for this example, this is used for analysis rather than for         # deciding whether to open or close a position         obs_no_pos = deepcopy(obs)         obs_no_pos[\"pos_val\"] = np.array([[0.0,0.0]]*20, dtype=np.float32)         m = [1 if \"Open\" in x else 0 for x in action_labels]         m[0] = 1         obs_no_pos[\"mask\"] = np.array(m, dtype=np.float32)         act_no_pos, _, act_prob_no_pos = trainer.compute_single_action(             obs_no_pos,             explore=False,             full_fetch=True,         )          # calculate value function as if agent had not opened a trade         # note that for this example, this is used for analysis rather than for         # deciding whether to open or close a position         input_dict = {SampleBatch.OBS: obs_no_pos}         acd = AgentConnectorDataType(\"0\", \"0\", input_dict)         pp.reset(env_id=\"0\")         ac_o = pp([acd])[0]         observation = ac_o.data[SampleBatch.OBS]         input_dict = {             \"default_policy/obs:0\": observation.reshape((1, -1)),         }         v = value_func.eval(input_dict)[0]         val_no_pos.append(v)          # take step based on action that takes positions into account         obs, reward, done, _, _ = env.step(act_pos)                  # store log-likelihoods         actions_pos.append(act_prob_pos[\"action_dist_inputs\"])         actions_no_pos.append(act_prob_no_pos[\"action_dist_inputs\"])          if done:             break       res, cp = env.get_results() <pre>        -1:       win rate     0% (0) &lt;   40%\n</pre> <pre>  0%|          | 0/10000 [00:00&lt;?, ?it/s] 26%|\u2588\u2588\u258c       | 2615/10000 [04:56&lt;23:06,  5.33it/s]Exception in thread Thread-6:\nTraceback (most recent call last):\n  File \"/workspaces/releat/.venv/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/workspaces/releat/.venv/lib/python3.10/site-packages/ray/rllib/execution/learner_thread.py\", line 74, in run\n    self.step()\n  File \"/workspaces/releat/.venv/lib/python3.10/site-packages/ray/rllib/execution/multi_gpu_learner_thread.py\", line 143, in step\n    buffer_idx, released = self.ready_tower_stacks_buffer.get()\n  File \"/workspaces/releat/.venv/lib/python3.10/site-packages/ray/rllib/execution/minibatch_buffer.py\", line 48, in get\n    self.buffers[self.idx] = self.inqueue.get(timeout=self.timeout)\n  File \"/workspaces/releat/.venv/lib/python3.10/queue.py\", line 179, in get\n    raise Empty\n_queue.Empty\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 9983/10000 [18:59&lt;00:01,  8.76it/s]\n</pre> In\u00a0[7]: Copied! <pre># convert steps to minutes\ncp[\"hold_time\"] = cp[\"hold_time\"] / 6\n# multiply by lot size\ncp[\"curr_val\"] = cp[\"curr_val\"] * cp[\"pos_size\"]\nwith pd.option_context(\"display.max_columns\",None,\"display.width\",1000,):\n    cols = [\"open_time\",\"curr_time\",\"pos_dir\",\"pos_size\",\"curr_val\",\"hold_time\",]\n    display(cp[cols].head(10))\n    print(f\"total pips: {cp['curr_val'].sum()}\", end=\"\\n\\n\")\n</pre> # convert steps to minutes cp[\"hold_time\"] = cp[\"hold_time\"] / 6 # multiply by lot size cp[\"curr_val\"] = cp[\"curr_val\"] * cp[\"pos_size\"] with pd.option_context(\"display.max_columns\",None,\"display.width\",1000,):     cols = [\"open_time\",\"curr_time\",\"pos_dir\",\"pos_size\",\"curr_val\",\"hold_time\",]     display(cp[cols].head(10))     print(f\"total pips: {cp['curr_val'].sum()}\", end=\"\\n\\n\") open_time curr_time pos_dir pos_size curr_val hold_time 0 2023-06-29 20:09:23 2023-06-29 20:19:03 -1 1 2.099819 9.666667 1 2023-06-29 20:09:23 2023-06-29 20:19:13 -1 1 2.400226 9.833333 2 2023-06-29 20:19:23 2023-06-29 20:22:53 -1 1 -0.299864 3.500000 3 2023-06-29 20:19:23 2023-06-29 20:23:13 -1 1 -1.899653 3.833333 4 2023-06-29 20:23:23 2023-06-29 21:23:33 -1 2 -4.001962 60.166667 5 2023-06-29 21:23:43 2023-06-29 21:27:23 -1 1 1.000709 3.666667 6 2023-06-29 21:23:43 2023-06-29 21:38:43 -1 1 0.700302 15.000000 7 2023-06-29 21:38:53 2023-06-29 21:40:33 -1 1 0.500030 1.666667 8 2023-06-29 21:38:53 2023-06-29 21:40:43 -1 1 1.000709 1.833333 9 2023-06-29 21:40:53 2023-06-29 21:50:23 -1 1 1.799411 9.500000 <pre>total pips: -150.10326995849616\n\n</pre> In\u00a0[8]: Copied! <pre># format log likelihoods when open positions are taken into account\nactions_pos = pd.DataFrame(\n    actions_pos,\n    columns=[f\"{x}\" for x in action_labels],\n).replace(np.finfo(np.float32).min, np.nan)\nactions_pos = ((actions_pos * 100) // 1) / 100\n\n# format log likelihoods assuming no positions\nactions_no_pos = pd.DataFrame(\n    actions_no_pos,\n    columns=[f\"{x}\" for x in action_labels],\n).replace(np.finfo(np.float32).min, np.nan)\nactions_no_pos = ((actions_no_pos * 100) // 1) / 100\n\n# combine the two dataframes\n# when there are opened positions, log likehood is np.nan for further opening positions\n# we fill this with log likehoods from when we assume no position is open\nfor col in actions_pos.columns:\n    actions_pos[col].fillna(actions_no_pos[col],inplace=True)\n# create a separate column for the hold position log likelihood\nactions_pos['Hold(NP)'] = actions_no_pos['Hold']\n\n# re-ordering columns\ncols = deepcopy(action_labels)\ncols.insert(1,'Hold(NP)')\nactions_pos = actions_pos[cols]\n\n# get output of the value function\nval_pos = pd.DataFrame(val_pos, columns=[\"ValueFunc\"])\nval_no_pos = pd.DataFrame(val_no_pos, columns=[\"ValueFunc(NP)\"])\n\n# combine into single dataframe\nres = pd.concat([res, val_pos, val_no_pos, actions_pos], axis=1)\n\nwith pd.option_context(\"display.max_columns\",None,\"display.width\",1000,):\n    display(res.head(10))\n</pre> # format log likelihoods when open positions are taken into account actions_pos = pd.DataFrame(     actions_pos,     columns=[f\"{x}\" for x in action_labels], ).replace(np.finfo(np.float32).min, np.nan) actions_pos = ((actions_pos * 100) // 1) / 100  # format log likelihoods assuming no positions actions_no_pos = pd.DataFrame(     actions_no_pos,     columns=[f\"{x}\" for x in action_labels], ).replace(np.finfo(np.float32).min, np.nan) actions_no_pos = ((actions_no_pos * 100) // 1) / 100  # combine the two dataframes # when there are opened positions, log likehood is np.nan for further opening positions # we fill this with log likehoods from when we assume no position is open for col in actions_pos.columns:     actions_pos[col].fillna(actions_no_pos[col],inplace=True) # create a separate column for the hold position log likelihood actions_pos['Hold(NP)'] = actions_no_pos['Hold']  # re-ordering columns cols = deepcopy(action_labels) cols.insert(1,'Hold(NP)') actions_pos = actions_pos[cols]  # get output of the value function val_pos = pd.DataFrame(val_pos, columns=[\"ValueFunc\"]) val_no_pos = pd.DataFrame(val_no_pos, columns=[\"ValueFunc(NP)\"])  # combine into single dataframe res = pd.concat([res, val_pos, val_no_pos, actions_pos], axis=1)  with pd.option_context(\"display.max_columns\",None,\"display.width\",1000,):     display(res.head(10)) data_ind action_time action reward cum_rewards 0_port_ind 0_symbol 0_pip_val 0_max_short 0_max_long 0_pos_size 0_pos_dir 0_open_time 0_curr_time 0_hold_time 0_open_price 0_curr_price 0_curr_val ValueFunc ValueFunc(NP) Hold Hold(NP) EURUSD_Short_Open_1_0 EURUSD_Short_Close_1_0 EURUSD_Short_Open_2_0 EURUSD_Short_Close_2_0 EURUSD_Long_Open_1_0 EURUSD_Long_Close_1_0 EURUSD_Long_Open_2_0 EURUSD_Long_Close_2_0 0 188666.0 2023-06-29 20:09:23 3.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:09:23 0 1.08717 1.08717 -0.0 0.010523 0.010523 1.04 1.04 1.36 NaN 1.99 NaN -1.72 NaN -2.19 NaN 1 188667.0 2023-06-29 20:09:33 0.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:09:33 1 1.08717 1.08715 0.200272 -0.113023 0.074135 2.48 0.80 1.01 NaN 1.52 NaN -2.08 NaN -2.39 NaN 2 188668.0 2023-06-29 20:09:43 0.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:09:43 2 1.08717 1.08714 0.300407 0.069902 0.114597 3.81 0.81 0.96 NaN 1.65 NaN -2.13 NaN -2.26 NaN 3 188669.0 2023-06-29 20:09:53 0.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:09:53 3 1.08717 1.08715 0.200272 0.049964 0.046078 2.19 1.68 1.36 NaN 1.75 NaN -2.22 NaN -2.33 NaN 4 188670.0 2023-06-29 20:10:03 0.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:10:03 4 1.08717 1.08711 0.599623 0.097391 0.026966 3.64 1.10 1.03 NaN 2.17 NaN -1.25 NaN -1.73 NaN 5 188671.0 2023-06-29 20:10:13 0.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:10:13 5 1.08717 1.08714 0.300407 0.169100 0.049183 3.98 1.09 1.15 NaN 2.13 NaN -1.40 NaN -1.89 NaN 6 188672.0 2023-06-29 20:10:23 0.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:10:23 6 1.08717 1.08714 0.300407 0.130823 0.012156 3.98 0.74 0.81 NaN 2.11 NaN -0.78 NaN -1.46 NaN 7 188673.0 2023-06-29 20:10:33 0.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:10:33 7 1.08717 1.08715 0.200272 0.129577 0.011078 4.31 0.92 0.93 NaN 1.98 NaN -1.54 NaN -2.00 NaN 8 188674.0 2023-06-29 20:10:43 0.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:10:43 8 1.08717 1.08715 0.200272 0.184636 0.030934 4.69 1.26 1.10 NaN 2.06 NaN -1.87 NaN -2.11 NaN 9 188675.0 2023-06-29 20:10:53 0.0 0.0 0.0 0 EURUSD 0.0001 2 2 2 -1 2023-06-29 20:09:23 2023-06-29 20:10:53 9 1.08717 1.08715 0.200272 0.411321 0.169058 3.65 0.38 0.83 -3.37 1.80 -3.49 -1.49 NaN -1.99 NaN In\u00a0[9]: Copied! <pre># save file in folders which are ordered by checkpoint\nckpt = checkpoint_f[-1].split(\"/\")[-2].split(\"_\")[-1]\nprint(ckpt)\nos.makedirs(f'{config.paths.eval_dir}/{ckpt[:4]}', exist_ok=True)\n\nres.to_csv(\n    f'{config.paths.eval_dir}/{ckpt[:4]}/{ckpt[-2:]}-res.csv',\n    index=False,\n)\n\ncp.to_csv(\n    f'{config.paths.eval_dir}/{ckpt[:4]}/{ckpt[-2:]}-cp.csv',\n    index=False,\n)\n</pre> # save file in folders which are ordered by checkpoint ckpt = checkpoint_f[-1].split(\"/\")[-2].split(\"_\")[-1] print(ckpt) os.makedirs(f'{config.paths.eval_dir}/{ckpt[:4]}', exist_ok=True)  res.to_csv(     f'{config.paths.eval_dir}/{ckpt[:4]}/{ckpt[-2:]}-res.csv',     index=False, )  cp.to_csv(     f'{config.paths.eval_dir}/{ckpt[:4]}/{ckpt[-2:]}-cp.csv',     index=False, ) <pre>000924\n</pre> In\u00a0[10]: Copied! <pre>plot_data = res.copy()\nplot_data = plot_data[['action_time','cum_rewards','0_curr_price']]\nplot_data.set_index('action_time',inplace=True)\nplot_data.plot(secondary_y=\"0_curr_price\")\n</pre> plot_data = res.copy() plot_data = plot_data[['action_time','cum_rewards','0_curr_price']] plot_data.set_index('action_time',inplace=True) plot_data.plot(secondary_y=\"0_curr_price\") Out[10]: <pre>&lt;Axes: xlabel='action_time'&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/eval_rl_agent_simple/#evaluate-simple-rl-agent-t00001","title":"Evaluate simple RL agent (t00001)\u00b6","text":"<p>Evaluate a trained reinforcement learning agent model</p>"},{"location":"examples/eval_rl_agent_simple/#prerequisite","title":"PREREQUISITE\u00b6","text":"<p>There are a few steps that need to be run from terminal before this notebook is runnable. Each of these steps needs to be run in the docker container, so if you're starting from a local terminal, first run <code>docker exec -it &lt;container-name&gt; /bin/bash</code> where <code>&lt;container-name&gt;</code> is either<code>releat</code> or <code>releat-dc</code> depending on how you've set up your environment.</p> <ol> <li><p>Make sure that you have the data by downloading data from broker <code>/workspaces/releat/.venv/python /workspaces/releat/releat/workflows/download_mt5_data.py</code></p> </li> <li><p>Start services such as ray, aerospike, etc. <code>releat start</code></p> </li> <li><p>Build train data: <code>releat build-train-data t00001</code></p> </li> <li><p>Train rl agent: <code>releat train t00001</code></p> </li> </ol>"},{"location":"examples/eval_rl_agent_simple/#load-rl-agent-and-gym-environment","title":"Load RL agent and gym environment\u00b6","text":"<p>Load the configurations, trained model and gym environment in preparation for evaluation</p>"},{"location":"examples/eval_rl_agent_simple/#evaluate-rl-agent-on-environment","title":"Evaluate RL agent on environment\u00b6","text":""},{"location":"examples/eval_rl_agent_simple/#save-and-display-results","title":"Save and display results\u00b6","text":"<ul> <li><code>cp</code> is the closed positions, i.e. the trades made during the evaluation period</li> <li><code>res</code> is the metrics collected at each timestep</li> <li>files are saved in the folder that is denoted by the checkpoint</li> <li>commission are taken into account</li> </ul>"},{"location":"examples/eval_rl_agent_simple/#display-trades","title":"Display trades\u00b6","text":"<p>Column definitions</p> <ul> <li><code>open_time</code> - timestamp of when the position is opened in MT5's local time (which depends on the broker)</li> <li><code>curr_time</code> - timestamp of whent he position is closed</li> <li><code>pos_dir</code> - position direction, +1 indicates long, -1 indicates short</li> <li><code>pos_size</code>- position size in number of lots</li> <li><code>curr_val</code> - value of the position in pips (i.e. it has already been multiplied by lot size)</li> <li><code>hold_time</code> - how long the position is held in minutes</li> </ul>"},{"location":"examples/eval_rl_agent_simple/#timestep-metrics","title":"Timestep metrics\u00b6","text":"<p>For each timestep we collect the following metrics:</p> <ul> <li><code>data_ind</code> - aerospike table key for the last record in the observation</li> <li><code>action_time</code> - timestamp of the action in MT5's local time (which depends on the broker)</li> <li><code>action</code> - action index, see the variable <code>action_labels</code> above for the meaning of these indices</li> <li><code>reward</code> - gym reward at each timestep</li> <li><code>cum_rewards</code> - cumulative rewards</li> </ul> <p>For each tradable position in the portfolio, we capture the following columns (note for t00001) there's only one tradeable position</p> <ul> <li><code>port_ind</code> - index in the portfolio array used in the gym environment</li> <li><code>symbol</code> - trading instrument, i.e. EURUSD</li> <li><code>pip_val</code>- pip value, i.e. 1e-4</li> <li><code>max_short</code> - maximum size / number of lots of short positions</li> <li><code>max_long</code> - maximum size / number of lots of long positions</li> <li><code>pos_size</code> - current position size, 0 means no position, 1 means 1 lot, etc.</li> <li><code>pos_dir</code> - current position direction, 0 mean no position, +1 means long, -1 means short</li> <li><code>open_time</code> - timestamp of when the position was opened</li> <li><code>curr_time</code> - current timestamp</li> <li><code>hold_time</code> - how long the position has been held in terms of number of gym environment steps</li> <li><code>open_price</code> - price that the position was opened at</li> <li><code>curr_price</code> - current price of trading instrument</li> <li><code>curr_val</code> - current value of one lot of the position (i.e. it has NOT been multiplied by the <code>pos_size</code>)</li> </ul> <p>We also collect all the value functions and log-likelihoods of each action at each timestamp</p> <ul> <li><code>ValueFunc</code> - value function when opened positions are taken into account</li> <li><code>ValueFunc(NP)</code> - value function NOT taking opened positions into account</li> <li><code>Hold</code> - log likelihood of hold action when taking opened positions into account</li> <li><code>Hold(NP)</code> - log likelihood of hold action when not taking opened positions into account</li> <li><code>EURUSD_Short_Open_1_0</code> - log likehood of opening one short EURUSD position - will never be null because we also calculate the log likelihood as if no positions were open</li> <li><code>EURUSD_Short_Close_1_0</code>- log likehood of closing one short EURUSD position - null when there are no open positions</li> <li><code>EURUSD_Short_Open_2_0</code> - see <code>action_labels</code> section above for the explanation of these components</li> <li><code>EURUSD_Short_Close_2_0</code> - etc.</li> <li><code>EURUSD_Long_Open_1_0</code></li> <li><code>EURUSD_Long_Close_1_0</code></li> <li><code>EURUSD_Long_Open_2_0</code></li> <li><code>EURUSD_Long_Close_2_0</code></li> </ul>"},{"location":"examples/eval_rl_agent_simple/#visualise-trades-and-metrics","title":"Visualise Trades and Metrics\u00b6","text":""},{"location":"examples/make_gradient_feature/","title":"Make gradient feature","text":"In\u00a0[1]: Copied! <pre>from releat.utils.logging import get_logger\nfrom releat.utils.configs.config_builder import load_config\nfrom releat.data.pipeline import load_raw_tick_data\nfrom releat.data.cleaning import group_tick_data_by_time\nfrom releat.data.simple.stats import calc_gradient_feature\nfrom releat.data.cleaning import fill_trade_interval\nfrom releat.data.transformers import get_transform_params\nimport logging\nimport polars as pl\nimport pandas as pd\nlogger = get_logger(__name__, log_level=logging.INFO)\n</pre> from releat.utils.logging import get_logger from releat.utils.configs.config_builder import load_config from releat.data.pipeline import load_raw_tick_data from releat.data.cleaning import group_tick_data_by_time from releat.data.simple.stats import calc_gradient_feature from releat.data.cleaning import fill_trade_interval from releat.data.transformers import get_transform_params import logging import polars as pl import pandas as pd logger = get_logger(__name__, log_level=logging.INFO) In\u00a0[2]: Copied! <pre>config = load_config('t00001')\n</pre> config = load_config('t00001') In\u00a0[3]: Copied! <pre># Index of the feature group - in this case we want the 5m timeframe\nfeat_group_ind = 1\n\n# Index of the feature within the feature group\nfeat_ind = 3\n\nfeat_group = config.features[feat_group_ind]\nfc = feat_group.simple_features[feat_ind]\n\n# the simple config that defines a single feature (conversion to dict is for printing only)\ndict(fc)\n</pre> # Index of the feature group - in this case we want the 5m timeframe feat_group_ind = 1  # Index of the feature within the feature group feat_ind = 3  feat_group = config.features[feat_group_ind] fc = feat_group.simple_features[feat_ind]  # the simple config that defines a single feature (conversion to dict is for printing only) dict(fc) Out[3]: <pre>{'name': 'grad',\n 'broker': 'metaquotes',\n 'index': 3,\n 'symbol': 'EURUSD',\n 'timeframe': '5m',\n 'inputs': ['avg_price'],\n 'output_shape': (3, 1),\n 'timeframe_mode': 'rolling',\n 'kwargs': {'min_num': 10},\n 'fillna': 'zero',\n 'transforms': [TransformerConfig(name='clip', method='percentile', upper_lim=99.9, lower_lim=0.1, scale_factor=1, lam=None, mean=None, std=None, clip_min=None, clip_max=None, is_elementwise=None),\n  TransformerConfig(name='scale', method='PowerTransformer', upper_lim=None, lower_lim=None, scale_factor=None, lam=None, mean=None, std=None, clip_min=None, clip_max=None, is_elementwise=True),\n  TransformerConfig(name='scale', method='PiecewiseLinear', upper_lim=None, lower_lim=None, scale_factor=None, lam=None, mean=None, std=None, clip_min=None, clip_max=None, is_elementwise=None),\n  TransformerConfig(name='clip', method='value', upper_lim=3, lower_lim=-3, scale_factor=0.5, lam=None, mean=None, std=None, clip_min=None, clip_max=None, is_elementwise=None)]}</pre> In\u00a0[4]: Copied! <pre># load tick data\ndt = '2023-06-01'\nsymbol = fc.symbol\nbroker = fc.broker\ntick_df = load_raw_tick_data(config, broker, symbol, dt)\n\n# For this example, reduce sample size so that it runs quickly\ntick_df = tick_df.head(100_000)\n\n# Note that this is a polars dataframe\ntick_df.head(10)\n</pre> # load tick data dt = '2023-06-01' symbol = fc.symbol broker = fc.broker tick_df = load_raw_tick_data(config, broker, symbol, dt)  # For this example, reduce sample size so that it runs quickly tick_df = tick_df.head(100_000)  # Note that this is a polars dataframe tick_df.head(10) Out[4]: shape: (10, 7)bidasktime_mscavg_pricespreadtime_diffflagsf32f32datetime[ns]f32f32f32i321.07331.073512023-05-31 00:01:27.5151.0734050.00021null61.072441.073442023-05-31 00:01:27.5151.072940.0010.061.072441.073442023-05-31 00:02:00.0041.072940.00132.48899821.072441.073442023-05-31 00:03:17.0751.072940.00160.061.072441.073382023-05-31 00:03:47.9871.072910.0009430.91200141.072431.073382023-05-31 00:04:12.4461.0729050.0009524.45921.072381.073382023-05-31 00:04:42.9891.072880.00130.54299921.07241.073382023-05-31 00:04:59.9631.072890.0009816.97400161.072451.073432023-05-31 00:05:00.0791.072940.000980.11661.072671.073482023-05-31 00:05:00.4121.0730750.000810.3336 In\u00a0[5]: Copied! <pre>df_group = group_tick_data_by_time(config, feat_group_ind, tick_df)\n\n# Print some summary statistics\nsummary = df_group.agg(\n    [\n        pl.col(\"time_msc\").min().alias(\"min_datetime\"),\n        pl.col(\"time_msc\").max().alias(\"max_datetime\"),\n        pl.col(\"time_msc\").count().alias(\"num_ticks\"),\n        pl.col(\"avg_price\").last().alias(\"price\")\n    ]\n)\nsummary.head(10)\n</pre> df_group = group_tick_data_by_time(config, feat_group_ind, tick_df)  # Print some summary statistics summary = df_group.agg(     [         pl.col(\"time_msc\").min().alias(\"min_datetime\"),         pl.col(\"time_msc\").max().alias(\"max_datetime\"),         pl.col(\"time_msc\").count().alias(\"num_ticks\"),         pl.col(\"avg_price\").last().alias(\"price\")     ] ) summary.head(10) Out[5]: shape: (10, 5)time_mscmin_datetimemax_datetimenum_tickspricedatetime[ns]datetime[ns]datetime[ns]u32f322023-05-31 00:01:202023-05-31 00:01:24.5152023-05-31 00:06:14.865311.0732852023-05-31 00:01:302023-05-31 00:01:57.0042023-05-31 00:06:28.442311.073312023-05-31 00:01:402023-05-31 00:01:57.0042023-05-31 00:06:36.972341.073352023-05-31 00:01:502023-05-31 00:01:57.0042023-05-31 00:06:49.958371.073382023-05-31 00:02:002023-05-31 00:03:14.0752023-05-31 00:06:58.499401.0731152023-05-31 00:02:102023-05-31 00:03:14.0752023-05-31 00:06:58.499401.0731152023-05-31 00:02:202023-05-31 00:03:14.0752023-05-31 00:06:58.499401.0731152023-05-31 00:02:302023-05-31 00:03:14.0752023-05-31 00:07:20.782411.073132023-05-31 00:02:402023-05-31 00:03:14.0752023-05-31 00:07:20.782411.073132023-05-31 00:02:502023-05-31 00:03:14.0752023-05-31 00:07:45.719421.073125 <p>For this group by, note that:</p> <ul> <li>the column <code>time_msc</code> will be used as the index for building the feature</li> <li>the column <code>time_msc</code> increments in 10s, which is defined as the <code>trade_timeframe</code> parameter in <code>agents/t00001/agent_config.py</code></li> <li>the <code>min_datetime</code> and <code>max_datetime</code> look forward, i.e. for the timestamp <code>2023-05-31 00:01:20</code>, the maximum datetime in that group is <code>2023-05-31 00:06:14.865</code>. The time shift so that the feature done after the feature is build, i.e. later the timestamp for this group will be converted to <code>2023-05-31 00:06:20</code></li> </ul> In\u00a0[6]: Copied! <pre>feature_timeframe = fc.timeframe\ntrade_timeframe = config.raw_data.trade_timeframe\npip = config.symbol_info[config.symbol_info_index[fc.symbol]].pip\n\n# make the gradient feature\ndf = calc_gradient_feature(df_group, fc, pip)\n\ndf.head(10)\n</pre> feature_timeframe = fc.timeframe trade_timeframe = config.raw_data.trade_timeframe pip = config.symbol_info[config.symbol_info_index[fc.symbol]].pip  # make the gradient feature df = calc_gradient_feature(df_group, fc, pip)  df.head(10) Out[6]: shape: (10, 2)time_mscfeatdatetime[ns]f322023-05-31 00:01:2022.1165412023-05-31 00:01:3046.8460352023-05-31 00:01:4048.9934922023-05-31 00:01:5050.6374022023-05-31 00:02:0053.605082023-05-31 00:02:1053.605082023-05-31 00:02:2053.605082023-05-31 00:02:3050.9975052023-05-31 00:02:4050.9975052023-05-31 00:02:5047.326023 <p>As noted above, then clean the <code>time_msc</code> column by making sure its the correct type and adding a time offset. The timestamp label for each feature should refer to the right boundary, i.e. the gradient feature for <code>2023-05-31 00:06:20</code> refers to tick data that happens between <code>2023-05-31 00:01:20</code> (inclusive) and <code>2023-05-31 00:06:20</code> (excluding this timestamp)</p> <p>We also shift the feature by a trade time offset, which represents the lag or number of seconds that the agent makes a trade after the information is available. For this example, this lag is set to 3s.</p> In\u00a0[7]: Copied! <pre>df = df.with_columns(pl.col(\"time_msc\").dt.cast_time_unit(\"ns\"))\ndf = df.with_columns(pl.col(\"time_msc\").dt.offset_by(feature_timeframe)).with_columns(\n        pl.col(\"time_msc\").dt.offset_by(config.raw_data.trade_time_offset),\n    )\ndf.head(10)\n</pre> df = df.with_columns(pl.col(\"time_msc\").dt.cast_time_unit(\"ns\")) df = df.with_columns(pl.col(\"time_msc\").dt.offset_by(feature_timeframe)).with_columns(         pl.col(\"time_msc\").dt.offset_by(config.raw_data.trade_time_offset),     ) df.head(10)     Out[7]: shape: (10, 2)time_mscfeatdatetime[ns]f322023-05-31 00:06:2322.1165412023-05-31 00:06:3346.8460352023-05-31 00:06:4348.9934922023-05-31 00:06:5350.6374022023-05-31 00:07:0353.605082023-05-31 00:07:1353.605082023-05-31 00:07:2353.605082023-05-31 00:07:3350.9975052023-05-31 00:07:4350.9975052023-05-31 00:07:5347.326023 In\u00a0[8]: Copied! <pre># Fill any NAs in the dataset according to the feature config, also fill in any missing\n# timeframes\nprint(f\"feature set length before fill: {len(df)}\")\ndf = fill_trade_interval(df, trade_timeframe, fc.fillna)\nprint(f\"feature set length after fill: {len(df)}\")\n</pre> # Fill any NAs in the dataset according to the feature config, also fill in any missing # timeframes print(f\"feature set length before fill: {len(df)}\") df = fill_trade_interval(df, trade_timeframe, fc.fillna) print(f\"feature set length after fill: {len(df)}\") <pre>feature set length before fill: 12347\nfeature set length after fill: 12355\n</pre> In\u00a0[9]: Copied! <pre># The transforms are specified per feature config\nfc.transforms\n</pre> # The transforms are specified per feature config fc.transforms Out[9]: <pre>[TransformerConfig(name='clip', method='percentile', upper_lim=99.9, lower_lim=0.1, scale_factor=1, lam=None, mean=None, std=None, clip_min=None, clip_max=None, is_elementwise=None),\n TransformerConfig(name='scale', method='PowerTransformer', upper_lim=None, lower_lim=None, scale_factor=None, lam=None, mean=None, std=None, clip_min=None, clip_max=None, is_elementwise=True),\n TransformerConfig(name='scale', method='PiecewiseLinear', upper_lim=None, lower_lim=None, scale_factor=None, lam=None, mean=None, std=None, clip_min=None, clip_max=None, is_elementwise=None),\n TransformerConfig(name='clip', method='value', upper_lim=3, lower_lim=-3, scale_factor=0.5, lam=None, mean=None, std=None, clip_min=None, clip_max=None, is_elementwise=None)]</pre> In\u00a0[10]: Copied! <pre>cols = [x for x in df.columns if x != \"time_msc\"]\nfeats = df.select(cols).to_numpy()\n\nfeats_t = get_transform_params(config, feat_group_ind, feat_ind, feats)\nfeats_t[:10]\n</pre> cols = [x for x in df.columns if x != \"time_msc\"] feats = df.select(cols).to_numpy()  feats_t = get_transform_params(config, feat_group_ind, feat_ind, feats) feats_t[:10] Out[10]: <pre>array([[0.44273528],\n       [0.9170695 ],\n       [0.95826584],\n       [0.9898029 ],\n       [1.0233682 ],\n       [1.0233682 ],\n       [1.0233682 ],\n       [0.99671125],\n       [0.99671125],\n       [0.9262774 ]], dtype=float32)</pre> In\u00a0[11]: Copied! <pre>feats_t = feats_t[:len(summary),0]\nsummary = summary.to_pandas()\nsummary[\"feature\"] = feats_t\nsummary.head(10)\n</pre> feats_t = feats_t[:len(summary),0] summary = summary.to_pandas() summary[\"feature\"] = feats_t summary.head(10) Out[11]: time_msc min_datetime max_datetime num_ticks price feature 0 2023-05-31 00:01:20 2023-05-31 00:01:24.515 2023-05-31 00:06:14.865 31 1.073285 0.442735 1 2023-05-31 00:01:30 2023-05-31 00:01:57.004 2023-05-31 00:06:28.442 31 1.073310 0.917069 2 2023-05-31 00:01:40 2023-05-31 00:01:57.004 2023-05-31 00:06:36.972 34 1.073350 0.958266 3 2023-05-31 00:01:50 2023-05-31 00:01:57.004 2023-05-31 00:06:49.958 37 1.073380 0.989803 4 2023-05-31 00:02:00 2023-05-31 00:03:14.075 2023-05-31 00:06:58.499 40 1.073115 1.023368 5 2023-05-31 00:02:10 2023-05-31 00:03:14.075 2023-05-31 00:06:58.499 40 1.073115 1.023368 6 2023-05-31 00:02:20 2023-05-31 00:03:14.075 2023-05-31 00:06:58.499 40 1.073115 1.023368 7 2023-05-31 00:02:30 2023-05-31 00:03:14.075 2023-05-31 00:07:20.782 41 1.073130 0.996711 8 2023-05-31 00:02:40 2023-05-31 00:03:14.075 2023-05-31 00:07:20.782 41 1.073130 0.996711 9 2023-05-31 00:02:50 2023-05-31 00:03:14.075 2023-05-31 00:07:45.719 42 1.073125 0.926277 In\u00a0[12]: Copied! <pre>summary.set_index(\"time_msc\",inplace=True)\nsummary = summary[[\"price\",\"feature\"]]\nsummary.iloc[800:1000].plot(secondary_y='price', figsize=(8, 5))\n</pre> summary.set_index(\"time_msc\",inplace=True) summary = summary[[\"price\",\"feature\"]] summary.iloc[800:1000].plot(secondary_y='price', figsize=(8, 5)) Out[12]: <pre>&lt;Axes: xlabel='time_msc'&gt;</pre>"},{"location":"examples/make_gradient_feature/#make-gradient-feature","title":"Make gradient feature\u00b6","text":"<p>Example of how a feature is built. This notebook goes through each step in the <code>build_features_by_dt</code> function (and its components) in <code>releat/data/pipeline.py</code> script</p>"},{"location":"examples/make_gradient_feature/#prerequisite-download-tick-data","title":"PREREQUISITE - Download tick data\u00b6","text":"<p>Before running this notebook, download tick data from brokers by running the following command from within the docker container:</p> <p><code>/.venv/bin/python /workspaces/releat/workflows/download_mt5_data.py</code></p> <p>Alternatively, you can run it from your local terminal and execute on your docker container, replace <code>&lt;container-name&gt;</code> with the name of the container, which should either be <code>releat</code> or <code>releat-dc</code> depending on how you set it up:</p> <p><code>docker exec -it &lt;container-name&gt; /.venv/bin/python /workspaces/releat/workflows/download_mt5_data.py</code></p>"},{"location":"examples/make_gradient_feature/#load-feature-config-and-data","title":"Load feature config and data\u00b6","text":"<ul> <li>For this example, see /agents/t00001/feature_config.py</li> <li>The load_config function validates configs via pydantic as well as combines all the other config files in the /agents/t00001 folder</li> </ul>"},{"location":"examples/make_gradient_feature/#make-feature","title":"Make Feature\u00b6","text":"<p>This is mostly taken from the <code>make_feature</code> function of <code>releat/data/pipeline.py</code></p>"},{"location":"examples/make_gradient_feature/#scale-and-transform-feature","title":"Scale and Transform Feature\u00b6","text":"<p>Note that this overwrites any existing scaling parameters in <code>data/agent/t00001/features/1_5m/3_grad/transforms</code></p>"},{"location":"examples/make_gradient_feature/#visualize-feature","title":"Visualize Feature\u00b6","text":"<p>For the purposes of visualising, the datasets are roughly joined together. i.e. the filled in dataset has more records than the initial summary, but in this example the differences are small and is ignored</p>"},{"location":"examples/mt5_api_data_extract/","title":"Data extraction via MT5 api","text":"In\u00a0[1]: Copied! <pre>from releat.utils.configs.constants import mt5_api_port_map\nfrom concurrent.futures import ThreadPoolExecutor\nfrom releat.workflows.service_manager import kill_processes, get_pids, stop_mt5\nfrom releat.utils.logging import get_logger\nfrom releat.data.extractor import download_tick_data\nimport logging\nfrom datetime import datetime\nlogger = get_logger(__name__, log_level=logging.INFO)\n</pre> from releat.utils.configs.constants import mt5_api_port_map from concurrent.futures import ThreadPoolExecutor from releat.workflows.service_manager import kill_processes, get_pids, stop_mt5 from releat.utils.logging import get_logger from releat.data.extractor import download_tick_data import logging from datetime import datetime logger = get_logger(__name__, log_level=logging.INFO) In\u00a0[2]: Copied! <pre># start date - datetime must be in this format\ndt0 = datetime.strptime(\"2023-09-06 10:00:00.000\",\"%Y-%m-%d %H:%M:%S.%f\")\n# end date - excludes boundary\ndt1 = datetime.strptime(\"2023-09-06 10:00:10.001\",\"%Y-%m-%d %H:%M:%S.%f\")\n# data_mode - either demo or live\ndata_mode = \"demo\"\n# check api - whether to check the connection, if failed, it will try to initialize the\n# connection. checking is skipped at inference\ncheck_api = False\n\ndl_args = []\n\nfor broker, port_map in mt5_api_port_map.items():\n    for symbol, port in port_map.items():\n        # general is the port used for other interactions with mt5, i.e. order and\n        # getting position\n        if symbol!='general':\n            dl_arg = [broker,symbol,dt0,dt1,data_mode,check_api]\n            dl_args.append(dl_arg)\n\nprint(\"An example of input list to the download_tick_data function:\")  \nprint(dl_args[0])\n</pre> # start date - datetime must be in this format dt0 = datetime.strptime(\"2023-09-06 10:00:00.000\",\"%Y-%m-%d %H:%M:%S.%f\") # end date - excludes boundary dt1 = datetime.strptime(\"2023-09-06 10:00:10.001\",\"%Y-%m-%d %H:%M:%S.%f\") # data_mode - either demo or live data_mode = \"demo\" # check api - whether to check the connection, if failed, it will try to initialize the # connection. checking is skipped at inference check_api = False  dl_args = []  for broker, port_map in mt5_api_port_map.items():     for symbol, port in port_map.items():         # general is the port used for other interactions with mt5, i.e. order and         # getting position         if symbol!='general':             dl_arg = [broker,symbol,dt0,dt1,data_mode,check_api]             dl_args.append(dl_arg)  print(\"An example of input list to the download_tick_data function:\")   print(dl_args[0]) <pre>An example of input list to the download_tick_data function:\n['metaquotes', 'EURUSD', datetime.datetime(2023, 9, 6, 10, 0), datetime.datetime(2023, 9, 6, 10, 0, 10, 1000), 'demo', False]\n</pre> In\u00a0[3]: Copied! <pre># show example output of function\ndownload_tick_data(*dl_args[1])\n</pre> # show example output of function download_tick_data(*dl_args[1]) <pre>2023-11-04 10:35:44,096  INFO   releat.data.extractor  |  metaquotes AUDJPY 2023-09-06 10:00:00.000000 &gt;&gt; 2023-09-06 10:00:10.001000       65 ticks\n</pre> Out[3]: <pre>{'broker': 'metaquotes',\n 'symbol': 'AUDJPY',\n 'tick_df':        bid     ask  last  volume                time_msc  flags  volume_real\n 0   94.084  94.093   0.0       0 2023-09-06 10:00:00.016    130          0.0\n 1   94.083  94.092   0.0       0 2023-09-06 10:00:00.063    134          0.0\n 2   94.085  94.093   0.0       0 2023-09-06 10:00:00.116    134          0.0\n 3   94.085  94.094   0.0       0 2023-09-06 10:00:00.151      4          0.0\n 4   94.087  94.095   0.0       0 2023-09-06 10:00:00.174    134          0.0\n ..     ...     ...   ...     ...                     ...    ...          ...\n 60  94.071  94.079   0.0       0 2023-09-06 10:00:09.196    130          0.0\n 61  94.071  94.080   0.0       0 2023-09-06 10:00:09.345      4          0.0\n 62  94.074  94.082   0.0       0 2023-09-06 10:00:09.628    134          0.0\n 63  94.074  94.083   0.0       0 2023-09-06 10:00:09.677      4          0.0\n 64  94.075  94.084   0.0       0 2023-09-06 10:00:09.745    134          0.0\n \n [65 rows x 7 columns]}</pre> In\u00a0[4]: Copied! <pre>pool = ThreadPoolExecutor(len(dl_args))\n</pre> pool = ThreadPoolExecutor(len(dl_args)) In\u00a0[\u00a0]: Copied! <pre>%%timeit\nfor dl_arg in dl_args:\n    download_tick_data(*dl_arg)\n</pre> %%timeit for dl_arg in dl_args:     download_tick_data(*dl_arg) In\u00a0[\u00a0]: Copied! <pre>%%timeit\nlist(pool.map(lambda p: download_tick_data(*p), dl_args))\n</pre> %%timeit list(pool.map(lambda p: download_tick_data(*p), dl_args)) In\u00a0[5]: Copied! <pre># kill mt5\nstop_mt5()\n</pre> # kill mt5 stop_mt5() <pre>2023-11-04 10:35:52,946  INFO   releat.workflows.service_manager  |  process id: 728095 killed\n2023-11-04 10:35:52,947  INFO   releat.workflows.service_manager  |  process id: 728698 killed\n2023-11-04 10:35:52,948  INFO   releat.workflows.service_manager  |  MetaTrader5 stopped: process ids [728095, 728698] killed\n</pre> In\u00a0[6]: Copied! <pre># kill mt5 api process ids\npids = get_pids(\"wineserver\")\nkill_processes(pids)\nprint(f\"mt5 apis stopped - process ids {pids} killed\")\n\n# kill wine processes\npids = get_pids(\"python.exe\")\nkill_processes(pids)\n</pre> # kill mt5 api process ids pids = get_pids(\"wineserver\") kill_processes(pids) print(f\"mt5 apis stopped - process ids {pids} killed\")  # kill wine processes pids = get_pids(\"python.exe\") kill_processes(pids) <pre>2023-11-04 10:35:55,953  INFO   releat.workflows.service_manager  |  process id: 727954 killed\nmt5 apis stopped - process ids [727954] killed\n2023-11-04 10:35:55,962  INFO   releat.workflows.service_manager  |  process id: 728002 killed\n2023-11-04 10:35:55,965  INFO   releat.workflows.service_manager  |  process id: 728057 killed\n2023-11-04 10:35:55,967  INFO   releat.workflows.service_manager  |  process id: 728152 killed\n2023-11-04 10:35:55,968  INFO   releat.workflows.service_manager  |  process id: 728248 killed\n2023-11-04 10:35:55,970  INFO   releat.workflows.service_manager  |  process id: 728283 killed\n2023-11-04 10:35:55,971  INFO   releat.workflows.service_manager  |  process id: 728343 killed\n2023-11-04 10:35:55,972  INFO   releat.workflows.service_manager  |  process id: 728380 killed\n2023-11-04 10:35:55,973  INFO   releat.workflows.service_manager  |  process id: 728444 killed\n2023-11-04 10:35:55,974  INFO   releat.workflows.service_manager  |  process id: 728485 killed\n2023-11-04 10:35:55,974  INFO   releat.workflows.service_manager  |  process id: 728558 killed\n2023-11-04 10:35:55,975  INFO   releat.workflows.service_manager  |  process id: 728595 killed\n2023-11-04 10:35:55,976  INFO   releat.workflows.service_manager  |  process id: 728665 killed\n2023-11-04 10:35:55,979  INFO   releat.workflows.service_manager  |  process id: 728755 killed\n2023-11-04 10:35:55,981  INFO   releat.workflows.service_manager  |  process id: 728844 killed\n2023-11-04 10:35:55,983  INFO   releat.workflows.service_manager  |  process id: 728879 killed\n2023-11-04 10:35:55,984  INFO   releat.workflows.service_manager  |  process id: 728960 killed\n2023-11-04 10:35:55,986  INFO   releat.workflows.service_manager  |  process id: 728991 killed\n2023-11-04 10:35:55,987  INFO   releat.workflows.service_manager  |  process id: 729070 killed\n2023-11-04 10:35:55,987  INFO   releat.workflows.service_manager  |  process id: 729105 killed\n2023-11-04 10:35:55,988  INFO   releat.workflows.service_manager  |  process id: 729186 already killed\n</pre>"},{"location":"examples/mt5_api_data_extract/#data-extraction-via-mt5-api","title":"Data extraction via MT5 api\u00b6","text":"<p>Each trading symbol has its own process for the wine mt5 api. This is so that in production, we're able to reduce latency (at the cost of higher memory consumption) by pulling from one or more brokers in parallel.</p>"},{"location":"examples/mt5_api_data_extract/#prerequisite-start-mt5-apis","title":"PREREQUISITE - start mt5 apis\u00b6","text":"<p>Before running this notebooks, first launch the mt5 apis from the docker terminal using the command:</p> <p><code>releat launch-all-mt5-apis</code></p> <p>Alternatively, you can run it from your local terminal and execute on your docker container, replace <code>&lt;container-name&gt;</code> with the name of the container, which should either be <code>releat</code> or <code>releat-dc</code> depending on how you set it up:</p> <p><code>docker exec -it &lt;container-name&gt; releat launch-all-mt5-apis</code></p> <p>Launching wine processes from jupyter notebooks is unstable and mostly fails to start</p>"},{"location":"examples/mt5_api_data_extract/#compare-data-extraction-speed","title":"Compare data extraction speed\u00b6","text":"<p>When run in parallel, the time taken to download data is at least 2x as fast as compared to in sequence. Note results will vary depending on internet speed and whether results are cached</p>"},{"location":"examples/mt5_api_data_extract/#clean-processes-by-deleting-mt5-instances-and-apis","title":"Clean processes by deleting MT5 instances and apis\u00b6","text":""},{"location":"getting_started/basic_usage/","title":"Basic Usage","text":"<p>This is an example of how to train and deploy a simple agent that trade EURUSD in a metaquotes demo account. The configurations for this agent can be found in <code>agents/t00001</code> and a more detailed explanation be found in the configuration documentation.</p> <p>Note: - example only works Monday to Friday GMT+2/3 due to metaquotes not allowing data extract or trades on weekend demo accounts. - Steps 4+ are blocking processes, so open a new terminal to run in parallel.</p>"},{"location":"getting_started/basic_usage/#1-connect-to-running-docker-container","title":"1) Connect to running docker container","text":"<p>If opening a new terminal, connect interactively to the running docker container:</p> <pre><code>docker exec -it releat /bin/bash\n</code></pre> <p>Note: - This assumes that the running container is called <code>releat</code></p>"},{"location":"getting_started/basic_usage/#2-start-services","title":"2) Start services","text":"<p>Starts services necessary to train, deploy and monitor the reinforcement learning trading agent: - Aerospike: In-memory database to store features and hyperparameters - Ray: Manages compute and provides RL training and inference logic - MT5: Download data and trade for forex and futures (depending on broker) - Tensorboard: Monitor RL training progress - Redis: In memory cache for storing RL predictions</p> <p>The <code>launch-mt5-api</code> command launched a flask api  in wine to interact with MetaTrader5. It is broker specific because in the future, this architecture should allow for multi-broker strategies.</p> <p>IMPORTANT: If it is the first time starting up the docker container, or if it has been rebuilt, log in the your MT5 account manually and click the allow autotrading button. If not, steps 3+ will not work.</p> <pre><code>releat start\nreleat launch-mt5-api metaquotes general\n</code></pre> <p>Note: - A demo metaquotes account can be found in the <code>releat/utils/configs/constants.py</code>. Alternatively you can create new credentials from the metaquotes website</p>"},{"location":"getting_started/basic_usage/#3-build-training-data","title":"3) Build training data","text":"<p>Build the features defined by the <code>feature_config.py</code> script and upload to Aerospike.</p> <pre><code>releat build-train-data t00001\n</code></pre> <p>The <code>t00001</code> config creates two feature groups: - one group for 30s timeframe - one group for 5m timeframe</p> <p>The features within the 30s timeframe feature group include: - average price - modified one-hot encoding of the different flag types - average spread</p> <p>The features within the 5m timeframe include: - average price - min price - max price - gradient of ticks</p> <p>Features are built from tick data and are scaled by: - clipping by percentile - power transformer - linear scaling - clipping for extreme values</p> <p>These features are then updated to Aerospike where the key is: <code>(&lt;environment&gt;, &lt;agent_version&gt;, &lt;integer&gt;)</code>, i.e. <code>('prod','t00001',10000)</code> The value for this example is:</p> <pre><code>{\n    \"date\": str,\n    \"30s\": list,\n    \"5m\": list,\n    \"price\": list\n}\n</code></pre> <p>The price here indicates the bid-ask for EURUSD for the next 2s. It is not as used an input into the training model, rather it is used to simulate slippage in the gym environment.</p>"},{"location":"getting_started/basic_usage/#4-train-model","title":"4) Train model","text":"<p>In this step, the RL model is defined in <code>agent_model.py</code>: - a class that extend Ray's rllib's Tensorflow model - simple example of gated residual network - allows arbitrary dict inputs - allows action masking (i.e. to block bad actions, i.e. a close action if no positions are open)</p> <p>The training process is defined in <code>agent_config.py</code>: - <code>gym_env</code> key defines the gym environments hyperparameters and is uploaded to aerospike so it can be dynamically changed during training (if necessary). It also define training hyperparameters such as number of episodes and training frequency. - keys with the prefix <code>rl_</code> map directly to Ray's rllib's training configurations</p> <pre><code>releat train t00001\n</code></pre> <p>Note: - If you do not have a gpu, in the <code>agent_config.py</code> file, change <code>agent_config['rl_resources']['num_gpus]</code> from 1 to 0. - Model performance can be tracked in tensorboard, where the default address is http://localhost:6006/ - Resource usage can be tracked by ray dashboard</p>"},{"location":"getting_started/basic_usage/#5-generate-signal","title":"5) Generate signal","text":"<p>Using the artifacts generated by the training process, this generate signal process is deployed to continuously: - extract data from MT5 - note if first run in a fresh install, manually log into MT5 account and click on the enable algortrading button. - build features - makes predictions by invoking the RL agent - pushes predictions to redis - loads the latest checkpoint</p> <p>The frequency of the prediction is controlled by the configs set in <code>agent_config.py</code></p> <pre><code>releat generate-signal t00001\n</code></pre>"},{"location":"getting_started/basic_usage/#6-launch-trader","title":"6) Launch trader","text":"<p>The trader is agent version agnostic (for now) and is deployed to: - gets the predictions from redis (in the future it will have capability to aggregate predictions from multiple different RL agents) - applies some risk logic (such as lot size scaling) - applies other operational logic (i.e. minimum position hold time, forced close at session close, etc.) - executes open or close actions for long or short positions</p> <pre><code>releat launch-trader\n</code></pre>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Linux or WSL (for windows machine, preferably Ubuntu 20.04)</li> </ul>"},{"location":"getting_started/installation/#installation_1","title":"Installation","text":""},{"location":"getting_started/installation/#1-clone-the-repositry","title":"1) Clone the repositry","text":"<pre><code>git clone https://github.com/releat215/releat.git\n</code></pre>"},{"location":"getting_started/installation/#2-navigate-to-repository-folder","title":"2) Navigate to repository folder","text":"<p>Assuming that the repository is cloned to the default destination, navigate to the repository.</p> <pre><code>cd releat\n</code></pre>"},{"location":"getting_started/installation/#3-build-or-download-docker-container","title":"3) Build or download docker container","text":"<p>Developing in the docker container is recommended because: - Trained RL agents can be easily deployed to multiple instances - Collaborators are working with the same stack and package versions - Provides utility for installing MetaTrader5 in wine</p> <p>The docker container can be build by:</p> <pre><code>docker build -t releat -f ./infrastructure/releat/Dockerfile .\n</code></pre> <p>Alternatively, the container has been pre-built and can be fetched from docker:</p> <pre><code>docker pull releat215/releat:1.0\n</code></pre> <p>Note: - Detailed notes on the components of the DockerFile are in the development notes</p>"},{"location":"getting_started/installation/#4-run-docker-container","title":"4) Run docker container","text":"<p>Run and connect to the docker container. Depending on your IDE, there are two main methods: - VSCode's Dev Container - connecting to a running docker container</p>"},{"location":"getting_started/installation/#vscodes-dev-container","title":"VSCode's Dev Container","text":"<p>The scripts for setting up VSCode's dev container is stored in <code>.devcontainer</code>. The following shows how to start up and connect to the dev container:</p> <p>GET PICTURE OF CONNECT TO DEV CONTAINER</p> <p>Note: - For more infromation on how to launch the dev container, see VSCode's documentation - Docker must be started - If you do not have gpus, deleted the line <code>'--gpus==all'</code> in the <code>runArgs</code> key of the <code>.devcontainer/devcontainer.json</code> file</p>"},{"location":"getting_started/installation/#connect-to-a-running-docker-container","title":"Connect to a running docker container","text":"<p>Firstly run the docker container:</p> <pre><code>docker run \\\n    --net host \\\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\\n    -v $(pwd):/workspaces/releat \\\n    -e DISPLAY \\\n    --gpus all \\\n    -it \\\n    --name releat \\\n    releat215/releat:1.0\n</code></pre> <p>Then you can connect to the running container:</p> <pre><code>docker exec -it releat /bin/bash\n</code></pre> <p>Note: - Detailed notes on the purpose of each argument can be found in the development notes - When running python scripts, make sure to navigate to correct folder and activate the environment: <code>cd /workspaces/releat &amp;&amp; source activate ./.venv</code></p>"},{"location":"getting_started/installation/#5-update-cli","title":"5) Update CLI","text":"<p>The CLI uses cached function. If you update the source code, the <code>releat</code> package needs to be rebuilt and re-installed to reflect updated / new functions:</p> <pre><code>source activate ./.venv\npoetry build\npip install --user ./dist/releat-0.0.1-py3-none-any.whl --force-reinstall --no-deps\npip uninstall releat -y\npoetry install\n</code></pre> <p>Note: - The final uninstall and install removes the installed wheel (with cached functions) and re-installs the package in editable mode so that futher edits can be made to the source code.</p>"},{"location":"getting_started/overview/","title":"Overview","text":"<p>discord server free</p>"},{"location":"troubleshooting/troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/troubleshooting/#metatrader5","title":"MetaTrader5","text":""},{"location":"troubleshooting/troubleshooting/#mt5-window-does-not-open","title":"MT5 window does not open","text":"<ol> <li>Check that when you run the docker container, you pass through the display with <code>-v /tmp/.X11-unix:/tmp/.X11-unix</code></li> <li>Restart your computer. Sometimes when open / closing / starting / stopping the container multiple times, some weird gui cache behaviour is triggered / locked and the container becomes unable to open new windows</li> <li>Delete container from docker and rebuild container, sometime a fresh install of MT5 just magically fixes the problem. You shouldn't lose any info / data because all the scripts and data available in your docker container mounted from the host.</li> </ol>"},{"location":"troubleshooting/troubleshooting/#cant-programmatically-log-in-to-mt5","title":"Can't programmatically log in to MT5","text":"<ol> <li>Manually log into MT5 account using the gui interface. When you first install MT5, the instance does not automatically connect to the metaquotes server (no idea why). The first log in must be manual, i.e. click to search through the list of brokers, type in your username and password. Subsequent connections to other brokers so work afterwards</li> </ol>"},{"location":"troubleshooting/troubleshooting/#cant-programmatically-download-data-trade-in-mt5","title":"Can't programmatically download data / trade in MT5","text":"<ol> <li>Check that you are logged in. If you cant log in see the question above.</li> <li>Check that the algotrading button is pressed. By default, the algotrading button is off</li> </ol>"}]}